{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python-headless\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_fscore_support, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score, jaccard_score, log_loss, fbeta_score\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.000001\n",
    "group_type = \"uti\"\n",
    "model_name = \"Xception\"\n",
    "baseDir = \"./datasets/uti/\"\n",
    "os.listdir(baseDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc, RocCurveDisplay,\n",
    "    precision_recall_fscore_support, matthews_corrcoef, cohen_kappa_score,\n",
    "    balanced_accuracy_score, jaccard_score, log_loss, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# -------------------------\n",
    "# Training curves (works for any class count)\n",
    "# -------------------------\n",
    "def save_training_metrics(history, results_dir):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(os.path.join(results_dir, \"training_accuracy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(os.path.join(results_dir, \"training_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Text dump\n",
    "    with open(os.path.join(results_dir, \"training_validation_metrics.txt\"), \"w\") as f:\n",
    "        f.write(\"Training and Validation Metrics Per Epoch\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        epochs = len(history.history['loss'])\n",
    "        for i in range(epochs):\n",
    "            acc = history.history['accuracy'][i]\n",
    "            loss = history.history['loss'][i]\n",
    "            val_acc = history.history.get('val_accuracy', [np.nan]*epochs)[i]\n",
    "            val_loss = history.history.get('val_loss', [np.nan]*epochs)[i]\n",
    "            f.write(f\"Epoch {i+1}:\\n\")\n",
    "            f.write(f\"  Training Accuracy: {acc:.4f}, Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "            f.write(f\"  Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix (with optional normalization)\n",
    "# -------------------------\n",
    "def save_confusion_matrix(y_true, y_pred, class_names, results_dir, normalize=False):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1e-12)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, \"confusion_matrix.png\" if not normalize else \"confusion_matrix_normalized.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# ROC (one-vs-rest) for multiclass; binary handled as a special case\n",
    "# -------------------------\n",
    "def save_roc_curves(y_true, y_probs, class_names, results_dir):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Binary case: standard single ROC\n",
    "    if n_classes == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "        plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "        plt.title('ROC Curve (Binary)')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, \"roc_curve.png\"))\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    # Multiclass: OVR curves + micro/macro AUC\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))  # shape (N, C)\n",
    "\n",
    "    # Compute per-class ROC + AUC\n",
    "    fpr_dict, tpr_dict, auc_dict = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "        auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
    "\n",
    "    # Micro-average\n",
    "    fpr_dict[\"micro\"], tpr_dict[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_probs.ravel())\n",
    "    auc_dict[\"micro\"] = auc(fpr_dict[\"micro\"], tpr_dict[\"micro\"])\n",
    "\n",
    "    # Macro-average\n",
    "    # Average the AUCs directly (simple macro-AUC)\n",
    "    auc_macro = np.mean([auc_dict[i] for i in range(n_classes)])\n",
    "    auc_dict[\"macro\"] = auc_macro\n",
    "\n",
    "    # Plot all\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Micro\n",
    "    plt.plot(fpr_dict[\"micro\"], tpr_dict[\"micro\"], lw=3,\n",
    "             label=f\"micro-average ROC (AUC = {auc_dict['micro']:.3f})\")\n",
    "    # Each class\n",
    "    for i, name in enumerate(class_names):\n",
    "        plt.plot(fpr_dict[i], tpr_dict[i], lw=1.5, label=f\"{name} (AUC = {auc_dict[i]:.3f})\")\n",
    "    # Chance\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "    plt.title('ROC Curves (One-vs-Rest)')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right', ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, \"roc_curves_multiclass.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Metrics (binary or multiclass)\n",
    "# -------------------------\n",
    "def _per_class_specificity(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Specificity for each class in multiclass (treat class i as positive, others as negative).\n",
    "    Returns array of shape (n_classes,).\n",
    "    \"\"\"\n",
    "    specs = []\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "    for i in range(n_classes):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - (tp + fn + fp)\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        specs.append(spec)\n",
    "    return np.array(specs)\n",
    "\n",
    "def save_classification_metrics(y_true, y_pred, y_probs, results_dir, class_names, topk=(2,3)):\n",
    "    \"\"\"\n",
    "    y_true: (N,) integer labels\n",
    "    y_pred: (N,) integer predictions\n",
    "    y_probs: (N, C) probabilities for each class\n",
    "    \"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Averages\n",
    "    # macro = unweighted mean across classes; weighted = class-frequency weighted\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Multiclass-capable metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    jaccard_macro = jaccard_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    jaccard_weighted = jaccard_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    logloss = log_loss(y_true, y_probs)\n",
    "\n",
    "    # Top-k\n",
    "    topk_scores = {}\n",
    "    for k in topk:\n",
    "        if y_probs.shape[1] >= k:\n",
    "            topk_scores[f\"top_{k}_accuracy\"] = top_k_accuracy_score(y_true, y_probs, k=k)\n",
    "\n",
    "    # Specificity (per class + macro)\n",
    "    specs = _per_class_specificity(y_true, y_pred, n_classes)\n",
    "    specificity_macro = np.nanmean(specs)\n",
    "\n",
    "    # Classification report (per class)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "\n",
    "    # Save text\n",
    "    with open(os.path.join(results_dir, \"classification_metrics.txt\"), \"w\") as f:\n",
    "        f.write(\"=== Macro / Weighted Averages ===\\n\")\n",
    "        f.write(f\"Precision (macro):  {precision_macro:.4f}\\n\")\n",
    "        f.write(f\"Recall (macro):     {recall_macro:.4f}\\n\")\n",
    "        f.write(f\"F1-Score (macro):   {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision (weighted): {precision_weighted:.4f}\\n\")\n",
    "        f.write(f\"Recall (weighted):    {recall_weighted:.4f}\\n\")\n",
    "        f.write(f\"F1-Score (weighted):  {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Balanced Accuracy:   {balanced_acc:.4f}\\n\")\n",
    "        f.write(f\"Specificity (macro): {specificity_macro:.4f}\\n\")\n",
    "        f.write(f\"Jaccard (macro):     {jaccard_macro:.4f}\\n\")\n",
    "        f.write(f\"Jaccard (weighted):  {jaccard_weighted:.4f}\\n\")\n",
    "        f.write(f\"MCC:                 {mcc:.4f}\\n\")\n",
    "        f.write(f\"Cohen's Kappa:       {kappa:.4f}\\n\")\n",
    "        f.write(f\"Log Loss:            {logloss:.4f}\\n\")\n",
    "        for k, v in topk_scores.items():\n",
    "            f.write(f\"{k.replace('_',' ').title()}: {v:.4f}\\n\")\n",
    "        f.write(\"\\n=== Per-Class Specificity ===\\n\")\n",
    "        for name, spec in zip(class_names, specs):\n",
    "            f.write(f\"{name}: {spec:.4f}\\n\")\n",
    "        f.write(\"\\n=== Classification Report ===\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "# -------------------------\n",
    "# Orchestrator\n",
    "# -------------------------\n",
    "def save_model_metrics(model, test_ds, results_dir, class_names):\n",
    "    \"\"\"\n",
    "    - Evaluates the model\n",
    "    - Collects y_true, y_probs, y_pred\n",
    "    - Saves confusion matrix, ROC curves, metrics\n",
    "    Works if test_ds yields (x, y) where y is one-hot or sparse ints.\n",
    "    \"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(\"Evaluating the model on test data...\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    with open(os.path.join(results_dir, \"testing_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Gather labels from dataset\n",
    "    ys = []\n",
    "    for _, y in test_ds:\n",
    "        ys.append(y.numpy())\n",
    "    y_true_raw = np.concatenate(ys, axis=0)\n",
    "\n",
    "    # Handle one-hot vs sparse\n",
    "    if y_true_raw.ndim == 2 and y_true_raw.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true_raw, axis=1)\n",
    "    else:\n",
    "        y_true = y_true_raw.squeeze().astype(int)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict(test_ds)\n",
    "    if y_probs.ndim == 1:  # binary model with a single sigmoid output\n",
    "        y_probs = np.stack([1 - y_probs, y_probs], axis=1)\n",
    "\n",
    "    # Predicted labels\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "    # Confusion matrices\n",
    "    save_confusion_matrix(y_true, y_pred, class_names, results_dir, normalize=False)\n",
    "    save_confusion_matrix(y_true, y_pred, class_names, results_dir, normalize=True)\n",
    "\n",
    "    # ROC curves (binary or multiclass automatically)\n",
    "    save_roc_curves(y_true, y_probs, class_names, results_dir)\n",
    "\n",
    "    # Metrics (binary or multiclass automatically)\n",
    "    save_classification_metrics(y_true, y_pred, y_probs, results_dir, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA requires that you don't have a seed.\n",
    "\n",
    "def get_images_and_classes():\n",
    "    # Load training and validation datasets\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        baseDir + \"train\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",  # Use categorical for multi-class classification\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        baseDir + \"dev\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Test dataset (assuming separate directory for test data)\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        baseDir + \"test\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No shuffling for test set\n",
    "    )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    for images, labels in train_ds.take(1):\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return (train_ds, val_ds, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomRotation(0.1)\n",
    "]) # Light Augmentation\n",
    "\n",
    "def augment_and_normalize(_train_ds, _val_ds, _test_ds):\n",
    "    internal_train_ds = _train_ds.map(lambda x, y: (normalization_layer(data_augmentation(x, training=True)), y))\n",
    "    internal_val_ds = _val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    internal_test_ds = _test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    output_train_ds = internal_train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    output_val_ds = internal_val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    output_test_ds = internal_test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return (output_train_ds, output_val_ds, output_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ResNet50V2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_model():\n",
    "    Xception = tf.keras.applications.Xception(input_shape = (224,224,3), include_top=False)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(224, 224, 3)),  # 🔹 Explicit input layer\n",
    "        Xception,\n",
    "        # 🌀 Replace Flatten with GlobalAveragePooling2D for better generalization\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        # 🧩 Add normalization and dropout for stability\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(3, activation='softmax')  # Final classification layer\n",
    "    ])\n",
    "\n",
    "    # Call model to initialize input shape\n",
    "    model.build(input_shape=(None, 224, 224, 3))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',  # Suitable for binary classification\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ANOVA_REPEATS = 5\n",
    "epochs = 35\n",
    "\n",
    "for index in range(1, ANOVA_REPEATS + 1):\n",
    "    print(\"REPEAT ROUND: \", index)\n",
    "\n",
    "    (train_ds, val_ds, test_ds, class_names) = get_images_and_classes()\n",
    "    (output_train_ds, output_val_ds, output_test_ds) = augment_and_normalize(train_ds, val_ds, test_ds)\n",
    "    model = create_and_compile_model()\n",
    "\n",
    "    history = model.fit(output_train_ds, epochs=epochs, validation_data=output_val_ds)\n",
    "\n",
    "    path = \"results/\" + group_type + \"/\" + model_name +\"/\"\n",
    "    name = group_type.upper() + \"_\" + model_name +\"_Round\" + str(index) # Rounds are retrains for ANOVA\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    results_dir = path + name + \"/\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    model.save(results_dir + name + \".keras\")\n",
    "\n",
    "    save_training_metrics(history, results_dir)\n",
    "    save_model_metrics(model, output_test_ds, results_dir, class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2763641,
     "sourceId": 4774750,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
